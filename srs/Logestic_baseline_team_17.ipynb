{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import\n",
        "\n",
        "import pandas\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
        "from sklearn.decomposition import PCA\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import scipy.spatial\n",
        "from textblob import TextBlob\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "import glob\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6bOC3pDd64J",
        "outputId": "3253f776-0eb6-446a-8df5-5620eb302060"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS37utYIMqme",
        "outputId": "6e74481c-a75c-438e-9230-f610482046ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T-xO4nxpLWwg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def processText(column,n):#to remove stop words,to convert into lowercase and root word\n",
        "    import re\n",
        "\n",
        "    corpus = []\n",
        "    for i in range(n):\n",
        "        review = re.sub('[^a-zA-Z]', ' ', column[i])\n",
        "        review = review.lower()\n",
        "        review = review.split()\n",
        "        ps = PorterStemmer()\n",
        "        all_stopwords = stopwords.words('english')\n",
        "        all_stopwords.remove('not')\n",
        "        review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
        "        review = ' '.join(review)\n",
        "        corpus.append(review)\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xMmAdQbLMwUi"
      },
      "outputs": [],
      "source": [
        "def embed(input):\n",
        "  return model(input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_2023data(df):\n",
        "  # print(df['datetime'])\n",
        "  df=df.dropna()\n",
        "  # print(\"pal\")\n",
        "  df = df[~(df['datetime'] >= '30-12-2022')]#dropping rows of year=2023\n",
        "\n",
        "  df=df.reset_index(drop=True)\n",
        "\n",
        "  # print(\"udrasht\")\n",
        "  \n",
        "  # print(df['datetime'])\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "-3wr_XjSfmHo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sentment(corpus):\n",
        "  sentimentColumns=[]\n",
        "  n=0\n",
        "  p=0\n",
        "  for content in corpus:\n",
        "    blob = TextBlob(content)\n",
        "    sentiment=blob.sentiment.polarity\n",
        "    if sentiment<0:\n",
        "      sentimentColumns.append(0)\n",
        "      n=n+1\n",
        "    else:\n",
        "      sentimentColumns.append(1)\n",
        "      p=p+1\n",
        "  print(n,p,\"sentement\")\n",
        "\n",
        "  return sentimentColumns\n"
      ],
      "metadata": {
        "id": "onRNbL2egBo9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_y_data(df):\n",
        "  n,m=df.shape\n",
        "  daily = df['close'].shift(-1)/df['close'] - 1\n",
        "  y = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    if daily[i] >= 0:\n",
        "      y[i] = 1 # up\n",
        "    else:\n",
        "      y[i] = 0 # down\n",
        "  print(len(y))\n",
        "  # y = np.insert(y, 0, 0)\n",
        "  y=y[:-1]\n",
        "  print(len(y))\n",
        "  headers = ['y_lables']\n",
        "  y_df = pd.DataFrame(y, columns=headers)\n",
        "  y_df['datetime']=df['datetime']\n",
        "  return y_df\n"
      ],
      "metadata": {
        "id": "55S9JoPbi1sa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(df):\n",
        "  scalar = StandardScaler()\n",
        "  scaled_df = scalar.fit_transform(df) #scaling the csv data\n",
        "  df=scaled_df\n",
        "  return df"
      ],
      "metadata": {
        "id": "Xh2rFRshj4XC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_PCA(titleColumn,fild=\"0\"):\n",
        "  text_vector=[]\n",
        "  for text in titleColumn:\n",
        "      sentence = text\n",
        "      message_embeddings = embed([sentence])\n",
        "      # message_embedding_snippet = \", \".join((str(x) for x in message_embeddings[:3]))\n",
        "      temp=np.array(message_embeddings[0])\n",
        "      temp=temp.T\n",
        "      text_vector.append(temp)\n",
        "  text_vector=np.array(text_vector)\n",
        "  scalar = StandardScaler()\n",
        "  scaled_data = scalar.fit_transform(text_vector)\n",
        "  pca = PCA(n_components = 30)\n",
        "  pca.fit(scaled_data)\n",
        "  data_pca = pca.transform(scaled_data)\n",
        "  print(\"pca\")\n",
        "  df_after_pca=pd.DataFrame(data_pca, columns = ['pca1'+fild,'pca2'+fild,'pca3'+fild,'pca4'+fild,'pca5'+fild,'pca6'+fild,'pca7'+fild,'pca8'+fild,'pca9'+fild,'pca10'+fild,'pca11'+fild,'pca12'+fild,'pca13'+fild,'pca14'+fild,'pca15'+fild,'pca16'+fild,'pca17'+fild,'pca18'+fild,'pca19'+fild,'pca20'+fild,'pca21'+fild,'pca22'+fild,'pca23'+fild,'pca24'+fild,'pca25'+fild,'pca26'+fild,'pca27'+fild,'pca28'+fild,'pca29'+fild,'pca30'+fild])\n",
        "  print(\"pca2\")\n",
        "  return df_after_pca\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "def concat_data(df,df_after_pca,dateColumn,sentementvalue,df_after_pca_summery,sentementvalue_of_summary):\n",
        "  column=['close', 'open', 'low', 'high', 'volume', 'evm',  'force_index', 'rsi', 'cci', 'macd-signal', 'atr', 'vwap','stx_7_3',\n",
        "  'Total Revenue', 'Cost of Revenue', 'Gross Profit', 'Normalized EBITDA',  'Operating Cash Flow', 'Investing Cash Flow', 'Financing Cash Flow',\n",
        "   'Free Cash Flow', 'Total Assets',  'Total Liabilities Net Minority Interest', 'Total Equity Gross Minority Interest', 'Total Capitalization']\n",
        "  # column=['close', 'open', 'low', 'high', 'volume', 'evm',  'force_index', 'rsi', 'cci', 'macd-signal', 'atr', 'vwap']\n",
        " \n",
        "  # print(df[0])\n",
        "  df=pd.DataFrame(df,columns=column)\n",
        "  # print(df)\n",
        "  new_data=pd.concat([dateColumn,df,df_after_pca,df_after_pca_summery],axis=1)\n",
        "  new_data['Sentiment']=sentementvalue\n",
        "  new_data['sentiment_summary']=sentementvalue_of_summary\n",
        "  return new_data\n"
      ],
      "metadata": {
        "id": "RsRo4rOEkQCQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_data(new_data,y_data):\n",
        "  x_train = new_data[(new_data['datetime'] <'2022-01-01')]\n",
        "  x_test = new_data[(new_data['datetime'] >='2022-01-01')]\n",
        "  y_train = y_data[(y_data['datetime'] <'2022-01-01')]\n",
        "  y_test = y_data[(y_data['datetime'] >='2022-01-01')]\n",
        "  print(\"train x: \",x_train.shape,\"train y: \",y_train.shape)\n",
        "  print(\"test x: \",x_test.shape,\"test y: \",y_test.shape)\n",
        "# 2022-01-01\n",
        "\n",
        "  #remove datetime column from data\n",
        "  x_train = x_train.drop('datetime', axis=1)\n",
        "  # file_train = file_train.drop('title', axis=1)\n",
        "  x_test = x_test.drop('datetime', axis=1)\n",
        "  y_train = y_train.drop('datetime', axis=1)\n",
        "  y_test = y_test.drop('datetime', axis=1)\n",
        "  return x_train,x_test,y_train,y_test\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "JUrUPdklmm4z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ML_Model(x_train,x_test,y_train,y_test):\n",
        "  # x_train['Volume'] = np.log(x_train['Volume']+0.5)\n",
        "  # x_test['Volume'] = np.log(x_test['Volume']+0.5)\n",
        "  baseline_LR = LogisticRegression(C = 1e10, tol=0.000000001, max_iter=100000)\n",
        "  baseline_LR.fit(x_train, y_train)\n",
        "      # print('Coefficients: \\n', baseline_LR.coef_)\n",
        "  print(\"training accracy: %.4f\" % baseline_LR.score(x_train, y_train))\n",
        "  print(\"test accuracy: %.4f\" % baseline_LR.score(x_test, y_test))\n",
        "  y_fit_LR = baseline_LR.predict(x_test)\n",
        "  # print(y_fit_LR)\n",
        "  # print(y_df_test)\n",
        "  print('Accuracy:', accuracy_score(y_test, y_fit_LR))\n",
        "  print('ROC AUC Score:', roc_auc_score(y_test, y_fit_LR))\n",
        "  print('F1 score:', f1_score(y_test, y_fit_LR))\n",
        "  print()\n",
        "  print('\\n clasification report:\\n', classification_report(y_test, y_fit_LR))\n",
        "  print('\\n confussion matrix:\\n',confusion_matrix(y_test, y_fit_LR))\n",
        "  "
      ],
      "metadata": {
        "id": "3CJNHOjVojNi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set the directory path\n",
        "dir_path = 'data/'\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "file_list = os.listdir(dir_path)\n",
        "\n",
        "# Filter only CSV files\n",
        "csv_files = [file for file in file_list if file.endswith('.csv')]\n",
        "for item in csv_files:\n",
        "    #read each file from data folder\n",
        "    df = pandas.read_csv(dir_path+\"/\"+item)\n",
        "  #   df = df.drop(['stx_7_3',\n",
        "  # 'Total Revenue', 'Cost of Revenue', 'Gross Profit', 'Normalized EBITDA',  'Operating Cash Flow', 'Investing Cash Flow', 'Financing Cash Flow',\n",
        "  #  'Free Cash Flow', 'Total Assets',  'Total Liabilities Net Minority Interest', 'Total Equity Gross Minority Interest', 'Total Capitalization'], axis=1)\n",
        "    # df['datetime'] = pd.to_datetime(df['datetime'], format='%d-%m-%Y')\n",
        "    #remove 2023 data from dataset\n",
        "\n",
        "    df=drop_2023data(df)\n",
        "    y_data=pd.DataFrame(columns=['datetime','y_lable'])\n",
        "    y_data['datetime']=df['datetime']\n",
        "    y_data['y_lable']=df['y_actual']\n",
        "    df=df.drop(['y_actual'],axis='columns')\n",
        "\n",
        "    #take title column value to apply Sentiment analysis and Pca\n",
        "    # take datetime column  value to create y lables of perticular data\n",
        "    titleColumn=df['title']\n",
        "    dateColumn=df['datetime']\n",
        "    summarycolumn=df['summary']\n",
        "\n",
        "    #preprocessing title column\n",
        "    corpus=processText(titleColumn,len(titleColumn))\n",
        "    df['title']=corpus\n",
        "    df = df.astype({'title':'string'})\n",
        "    sentementvalue=find_sentment(corpus)\n",
        "\n",
        "    #preprocessing title column\n",
        "    corpus_summary=processText(summarycolumn,len(summarycolumn))\n",
        "    df['summary']=corpus_summary\n",
        "    df = df.astype({'summary':'string'})\n",
        "    sentementvalue_of_summary=find_sentment(corpus_summary)\n",
        "\n",
        "  \n",
        "    # df=pd.get_dummies(df,columns=['sentiment'],drop_first=True)\n",
        "    \n",
        "    #create y lables for data \n",
        "    # y_data = find_y_data(df)\n",
        "\n",
        "    #dropping the date,title column from the data\n",
        "    print(df.shape)\n",
        "    df=df.drop(['datetime','title','summary'],axis='columns')\n",
        "    df=normalize_data(df)\n",
        "\n",
        "    df_after_pca=apply_PCA(titleColumn,\"title\")\n",
        "    df_after_pca_summery=apply_PCA(summarycolumn,\"summary\")\n",
        "   \n",
        "    \n",
        "    #concate news pca data with the ofiginal data\n",
        "    new_data=concat_data(df,df_after_pca,dateColumn,sentementvalue,df_after_pca_summery,sentementvalue_of_summary)\n",
        "\n",
        "    #divide data in test train\n",
        "\n",
        "    x_train,x_test,y_train,y_test=divide_data(new_data,y_data)\n",
        "    #Apply Model\n",
        "    ML_Model(x_train,x_test,y_train,y_test)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LitNOHAZfbx3",
        "outputId": "cb7cdcc7-1e78-44dd-bea9-1fff4ff554d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72 902 sentement\n",
            "145 829 sentement\n",
            "(974, 28)\n",
            "pca\n",
            "pca2\n",
            "pca\n",
            "pca2\n",
            "train x:  (724, 88) train y:  (724, 2)\n",
            "test x:  (250, 88) test y:  (250, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training accracy: 0.6533\n",
            "test accuracy: 0.4640\n",
            "Accuracy: 0.464\n",
            "ROC AUC Score: 0.4797349459598559\n",
            "F1 score: 0.5472972972972974\n",
            "\n",
            "\n",
            " clasification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.26      0.34       134\n",
            "         1.0       0.45      0.70      0.55       116\n",
            "\n",
            "    accuracy                           0.46       250\n",
            "   macro avg       0.47      0.48      0.45       250\n",
            "weighted avg       0.48      0.46      0.44       250\n",
            "\n",
            "\n",
            " confussion matrix:\n",
            " [[35 99]\n",
            " [35 81]]\n",
            "49 925 sentement\n",
            "22 952 sentement\n",
            "(974, 28)\n",
            "pca\n",
            "pca2\n",
            "pca\n",
            "pca2\n",
            "train x:  (724, 88) train y:  (724, 2)\n",
            "test x:  (250, 88) test y:  (250, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training accracy: 0.6340\n",
            "test accuracy: 0.5320\n",
            "Accuracy: 0.532\n",
            "ROC AUC Score: 0.5005141057772636\n",
            "F1 score: 0.016806722689075633\n",
            "\n",
            "\n",
            " clasification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.53      0.99      0.69       133\n",
            "         1.0       0.50      0.01      0.02       117\n",
            "\n",
            "    accuracy                           0.53       250\n",
            "   macro avg       0.52      0.50      0.35       250\n",
            "weighted avg       0.52      0.53      0.38       250\n",
            "\n",
            "\n",
            " confussion matrix:\n",
            " [[132   1]\n",
            " [116   1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QQ0sEsHTLWzU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# df = pandas.read_csv('AAPL_combine.csv')\n",
        "\n",
        "\n",
        "# df=df.dropna()\n",
        "# df = df[~(df['datetime'] >= '2023-01-01')]#dropping rows of year=2023\n",
        "\n",
        "# df=df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# print(df.columns)\n",
        "\n",
        "# titleColumn=df['title']\n",
        "# dateColumn=df['datetime']\n",
        "# titleColumn=list(titleColumn)\n",
        "\n",
        "# corpus=processText(titleColumn,len(titleColumn))#preprocessing title column\n",
        "# df['title']=corpus\n",
        "\n",
        "# df = df.astype({'title':'string'})\n",
        "# print(df.tail(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YeGz9GKzEsXI"
      },
      "outputs": [],
      "source": [
        "# df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QqlmHiL8hJ7P"
      },
      "outputs": [],
      "source": [
        "# titleColumn=df['title']\n",
        "# titleColumn[0]\n",
        "\n",
        "# # df=df.fillna(0)\n",
        "\n",
        "# # df=df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head(10)"
      ],
      "metadata": {
        "id": "jAxhAYfhAFU5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7PVaEwFOupL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from textblob import TextBlob"
      ],
      "metadata": {
        "id": "Ga3Y9dNKOv4S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentimentColumns=[]\n",
        "# for content in corpus:\n",
        "#     blob = TextBlob(content)\n",
        "#     sentiment=blob.sentiment.polarity\n",
        "#     if sentiment<0:\n",
        "#         sentimentColumns.append(\"negative\")\n",
        "#     else:\n",
        "#         sentimentColumns.append(\"positive\")\n",
        "\n",
        "# df['sentiment']=sentimentColumns"
      ],
      "metadata": {
        "id": "IY3spEHJOv-I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df.head(5))"
      ],
      "metadata": {
        "id": "4EZoGXqVOwBa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df=pd.get_dummies(df,columns=['sentiment'],drop_first=True)"
      ],
      "metadata": {
        "id": "chJG3e3aPJYo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.columns"
      ],
      "metadata": {
        "id": "HXVG2gkiVw1H"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['sentiment_positive'].value_counts()"
      ],
      "metadata": {
        "id": "FgcStn9kT-vd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df.head(5))"
      ],
      "metadata": {
        "id": "icyYrUsOPNgN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UJTDrCkPNjw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xHLd2EOiLW1w"
      },
      "outputs": [],
      "source": [
        "# n,m=df.shape\n",
        "# daily = df['close'].shift(-1)/df['close'] - 1\n",
        "# y = np.zeros(n)\n",
        "# for i in range(n):\n",
        "#   if daily[i] >= 0:\n",
        "#     y[i] = 1 # up\n",
        "#   else:\n",
        "#     y[i] = 0 # down\n",
        "# print(len(y))\n",
        "# y = np.insert(y, 0, 0)\n",
        "# y=y[:-1]\n",
        "# print(len(y))\n",
        "# headers = ['y_lables']\n",
        "# y_df = pd.DataFrame(y, columns=headers)\n",
        "# y_df['datetime']=df['datetime']\n",
        "# y_df.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kvuhkQHHBg5k"
      },
      "outputs": [],
      "source": [
        "# df.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LzuRcgFHBo5d"
      },
      "outputs": [],
      "source": [
        "# # df_title=df[['datetime','title']]\n",
        "# df=df.drop(['datetime','title'],axis='columns')#dropping the date column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jEJ-9yxrgKlU"
      },
      "outputs": [],
      "source": [
        "# scalar = StandardScaler()\n",
        "# scaled_df = scalar.fit_transform(df) #scaling the csv data\n",
        "\n",
        "# df=scaled_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.shape"
      ],
      "metadata": {
        "id": "YBrdD6PQLqEd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "U1OkQmI7NkiC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# text_vector=[]# will contain vector representation of titleColumn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EZJlRFCBNJes"
      },
      "outputs": [],
      "source": [
        "# # text_vector=[]\n",
        "# for text in titleColumn:\n",
        "#     sentence = text\n",
        "#     message_embeddings = embed([sentence])\n",
        "#     # message_embedding_snippet = \", \".join((str(x) for x in message_embeddings[:3]))\n",
        "#     temp=np.array(message_embeddings[0])\n",
        "#     temp=temp.T\n",
        "#     text_vector.append(temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LO3jjvT5O0QL"
      },
      "outputs": [],
      "source": [
        "# text_vector=np.array(text_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "O6mJCluaO3Dn"
      },
      "outputs": [],
      "source": [
        "# text_vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "s5nMVv2APVh3"
      },
      "outputs": [],
      "source": [
        "# # text_vector=np.array(text_vector)\n",
        "# scalar = StandardScaler()\n",
        "# scaled_data = scalar.fit_transform(text_vector) #scaling the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YI78ecE_PVqj"
      },
      "outputs": [],
      "source": [
        "# pca = PCA(n_components = 25)\n",
        "# pca.fit(scaled_data)\n",
        "# data_pca = pca.transform(scaled_data)\n",
        "# data_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Aw0__yWZPzn2"
      },
      "outputs": [],
      "source": [
        "# data_pca.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HOyASLNYQYNd"
      },
      "outputs": [],
      "source": [
        "# text_vector_dataframe=pd.DataFrame()\n",
        "# type(data_pca)\n",
        "# # df_news  =pd.DataFrame(((x,) for x in data_pca), columns=['title'])\n",
        "# df_news=pd.DataFrame(data_pca, columns = ['pca1','pca2','pca3','pca4','pca5','pca6','pca7','pca8','pca9','pca10','pca11','pca12','pca13','pca14','pca15','pca16','pca17','pca18','pca19','pca20','pca21','pca22','pca23','pca24','pca25'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_news"
      ],
      "metadata": {
        "id": "AYImrFzORb8M"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eexBYzukQYQ_"
      },
      "outputs": [],
      "source": [
        "# text_vector_dataframe.describe()\n",
        "# text_vector_dataframe['title']=data_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "K08q3QZMRCnT"
      },
      "outputs": [],
      "source": [
        "# column=['close', 'open', 'low', 'high', 'volume', 'evm',  'force_index', 'rsi', 'cci', 'macd-signal', 'atr', 'vwap', 'stx_7_3',\n",
        "#   'Total Revenue', 'Cost of Revenue', 'Gross Profit', 'Normalized EBITDA',  'Operating Cash Flow', 'Investing Cash Flow', 'Financing Cash Flow',\n",
        "#    'Free Cash Flow', 'Total Assets',  'Total Liabilities Net Minority Interest', 'Total Equity Gross Minority Interest', 'Total Capitalization','Sentiment']\n",
        "\n",
        "# df=pd.DataFrame(df,columns=column)\n",
        "# newDF=pd.concat([dateColumn,df,df_news],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YSd3ApEcQsZb"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tIckXIm8Q5BW"
      },
      "outputs": [],
      "source": [
        "# newDF.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1zxAj31FQ5EA"
      },
      "outputs": [],
      "source": [
        "# newDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# newDF.columns"
      ],
      "metadata": {
        "id": "uA1KCrreS3V6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhdna_axklbH"
      },
      "source": [
        "Start applying model after this.\n",
        "\n",
        "Take data before 2022-9-30 as training and after as testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "u_HjY-1DM1_W"
      },
      "outputs": [],
      "source": [
        "# file_train = newDF[(newDF['datetime'] <'2022-01-01')]\n",
        "# file_test = newDF[(newDF['datetime'] >='2022-01-01')]\n",
        "# y_df_train = y_df[(y_df['datetime'] <'2022-01-01')]\n",
        "# y_df_test = y_df[(y_df['datetime'] >='2022-01-01')]\n",
        "# print(\"train x: \",file_train.shape,\"train y: \",y_df_train.shape)\n",
        "# print(\"test x: \",file_test.shape,\"test y: \",y_df_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file_train = file_train.drop('datetime', axis=1)\n",
        "# # file_train = file_train.drop('title', axis=1)\n",
        "# file_test = file_test.drop('datetime', axis=1)\n",
        "# y_df_train = y_df_train.drop('datetime', axis=1)\n",
        "# y_df_test = y_df_test.drop('datetime', axis=1)\n",
        "# file_train"
      ],
      "metadata": {
        "id": "8CFCLvdPVxVG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_test"
      ],
      "metadata": {
        "id": "s6kTZOv0XDo_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_df_train"
      ],
      "metadata": {
        "id": "e7hNJEXRXDfG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_df_test"
      ],
      "metadata": {
        "id": "BUpH-wW3XDS9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # file_train.columns = file_train.iloc[0]\n",
        "# file_train = file_train.iloc[1:]\n",
        "\n",
        "# # file_test.columns = file_test.iloc[0]\n",
        "# file_test = file_test.iloc[1:]\n",
        "\n",
        "\n",
        "# # y_df_train.columns = y_df_train.iloc[0]\n",
        "# y_df_train = y_df_train.iloc[1:]\n",
        "\n",
        "\n",
        "# # y_df_test.columns = y_df_test.iloc[0]\n",
        "# y_df_test = y_df_test.iloc[1:]"
      ],
      "metadata": {
        "id": "tFZ3xyBKUAVg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(y_df_test)"
      ],
      "metadata": {
        "id": "ay7cmCaNU2nw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# from sklearn.metrics import f1_score\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# baseline_LR = LogisticRegression(C = 1e10, tol=0.000000001, max_iter=100000)\n",
        "# baseline_LR.fit(file_train, y_df_train)\n",
        "#     # print('Coefficients: \\n', baseline_LR.coef_)\n",
        "# print(\"training accracy: %.4f\" % baseline_LR.score(file_train, y_df_train))\n",
        "# print(\"test accuracy: %.4f\" % baseline_LR.score(file_test, y_df_test))\n",
        "# y_fit_LR = baseline_LR.predict(file_test)\n",
        "# # print(y_fit_LR)\n",
        "# # print(y_df_test)\n",
        "# print('Accuracy:', accuracy_score(y_df_test, y_fit_LR))\n",
        "# print('ROC AUC Score:', roc_auc_score(y_df_test, y_fit_LR))\n",
        "# print('F1 score:', f1_score(y_df_test, y_fit_LR))\n",
        "# print()\n",
        "# print('\\n clasification report:\\n', classification_report(y_df_test, y_fit_LR))\n",
        "# print('\\n confussion matrix:\\n',confusion_matrix(y_df_test, y_fit_LR))"
      ],
      "metadata": {
        "id": "kBWHjlXwT_Iy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn import svm\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# from sklearn.metrics import f1_score\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import classification_report\n",
        "# def calc_accuracy(pred, y):\n",
        "#     pred = np.matrix(pred).reshape(-1, 1)\n",
        "#     y = np.matrix(y).reshape(-1, 1)\n",
        "#     count = y.shape[0]\n",
        "#     correct = 0\n",
        "#     for i in range (count):\n",
        "#         if pred[i,0] == y[i, 0]:\n",
        "#             correct += 1\n",
        "#     return (correct * 1.0 / count)\n",
        "# baseline_LR = LogisticRegression(C = 1e10, tol=0.000000001, max_iter=100000)\n",
        "# baseline_LR.fit(file_train, y_df_train)\n",
        "# # Training acc\n",
        "# pred = baseline_LR.predict(file_train)\n",
        "# accuracy = calc_accuracy(pred, y_df_train)\n",
        "# print('The training accuracy of the Logistic Regression is ', accuracy)\n",
        "# # Test acc\n",
        "# y_hat = baseline_LR.predict(file_test)\n",
        "# test_accuracy = calc_accuracy(y_hat, y_df_test)\n",
        "# print('The test accuracy of the Logistic Regression is ', test_accuracy)\n",
        "# # Conf matrix\n",
        "# print('\\n Training clasification report:\\n', classification_report(y_df_train, pred))\n",
        "# print('\\n confusion matrix:\\n',confusion_matrix(y_df_train, pred))\n",
        "# print('\\n Test clasification report:\\n', classification_report(y_df_test, y_hat))\n",
        "# print('\\n confusion matrix:\\n',confusion_matrix(y_df_test, y_hat))\n",
        "\n",
        "# # Fit and report SVM\n",
        "# # c = 0.5\n",
        "# svm_rbf = svm.SVC(C=0.5, kernel='rbf',max_iter=50000)\n",
        "# svm_rbf.fit(file_train, y_df_train)\n",
        "# # Training acc\n",
        "# pred = svm_rbf.predict(file_train)\n",
        "# accuracy = calc_accuracy(pred, y_df_train)\n",
        "# print('The training accuracy of the SVM is ', accuracy)\n",
        "# # Test acc\n",
        "# y_hat = svm_rbf.predict(file_test)\n",
        "# test_accuracy = calc_accuracy(y_hat, y_df_test)\n",
        "# print('The test accuracy of the SVM is ', test_accuracy)\n",
        "# # Conf matrix\n",
        "# print('\\n Training classification report:\\n', classification_report(y_df_train, pred))\n",
        "# print('\\n confusion matrix:\\n',confusion_matrix(y_df_train, pred))\n",
        "# print('\\n Test classification report:\\n', classification_report(y_df_test, y_hat))\n",
        "# print('\\n confusion matrix:\\n',confusion_matrix(y_df_test, y_hat))"
      ],
      "metadata": {
        "id": "RSApvY4oC7Jw"
      },
      "execution_count": 51,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}